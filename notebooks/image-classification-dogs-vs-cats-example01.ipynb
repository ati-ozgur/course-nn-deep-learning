{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import os\n",
    "from mxnet.gluon.data.vision import  transforms\n",
    "from mxnet import gluon, autograd, ndarray\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import gluon, init, nd\n",
    "import time\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, utils as gutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pylab import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "CPU_COUNT = cpu_count()\n",
    "CPU_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"..\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"dogs-vs-cats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = os.path.join(data_folder, dataset_name,\"train\")\n",
    "validation_path = os.path.join(data_folder, dataset_name,\"validation\")\n",
    "test_path = os.path.join(data_folder, dataset_name,\"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 224\n",
    "image_height = 224\n",
    "from PIL import Image, ImageFilter\n",
    "def resize_images(directory_path):\n",
    "    files = []\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(directory_path):\n",
    "        for file in f:\n",
    "            if '.jpg' in file:\n",
    "                files.append(os.path.join(r, file))\n",
    "\n",
    "    for imagefilename in files:\n",
    "        im1 = Image.open(imagefilename)\n",
    "        im_resized = im1.resize((image_width, image_height), Image.NEAREST)\n",
    "        im_resized.save(imagefilename)\n",
    "        print(imagefilename)\n",
    "\n",
    "\n",
    "#resize_images(training_path)\n",
    "#resize_images(validation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mx.gluon.data.vision.datasets.ImageFolderDataset(training_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = mx.gluon.data.vision.datasets.ImageFolderDataset(validation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size, resize=None):\n",
    "    transformer = []\n",
    "    if resize:\n",
    "        transformer += [mx.gluon.data.vision.transforms.Resize(resize)]\n",
    "    transformer += [mx.gluon.data.vision.transforms.ToTensor()]\n",
    "    transformer = mx.gluon.data.vision.transforms.Compose(transformer)\n",
    "    num_workers = 0 \n",
    "    train_iter = mx.gluon.data.DataLoader(\n",
    "        train_dataset.transform_first(transformer), batch_size, shuffle=True,\n",
    "        num_workers=num_workers)\n",
    "    validation_iter = mx.gluon.data.DataLoader(\n",
    "        validation_dataset.transform_first(transformer), batch_size, shuffle=False,\n",
    "        num_workers=num_workers)\n",
    "    return train_iter, validation_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, validation_iter= load_data(batch_size, resize=224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_idx = 539\n",
    "sample = train_dataset[sample_idx]\n",
    "sample_data = sample[0]\n",
    "label = sample_data[1]\n",
    "data_type = sample_data.dtype\n",
    "data_shape = sample_data.shape\n",
    "\n",
    "imshow(sample_data.asnumpy(), cmap='gray')\n",
    "print(f\"Data type: {data_type}, data size: {data_shape} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch_idx, (data, label) in enumerate(train_data_loader):\n",
    "#    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def construct_net():\n",
    "    num_fc = 512\n",
    "    num_outputs = 2\n",
    "    net = nn.Sequential()\n",
    "    # Alexnet example\n",
    "    # Here, we use a larger 11 x 11 window to capture objects. At the same time,\n",
    "    # we use a stride of 4 to greatly reduce the height and width of the output.\n",
    "    # Here, the number of output channels is much larger than that in LeNet\n",
    "    net.add(nn.Conv2D(96, kernel_size=11, strides=4, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        # Make the convolution window smaller, set padding to 2 for consistent\n",
    "        # height and width across the input and output, and increase the\n",
    "        # number of output channels\n",
    "        nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        # Use three successive convolutional layers and a smaller convolution\n",
    "        # window. Except for the final convolutional layer, the number of\n",
    "        # output channels is further increased. Pooling layers are not used to\n",
    "        # reduce the height and width of input after the first two\n",
    "        # convolutional layers\n",
    "        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n",
    "        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n",
    "        nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        # Here, the number of outputs of the fully connected layer is several\n",
    "        # times larger than that in LeNet. Use the dropout layer to mitigate\n",
    "        # overfitting\n",
    "        nn.Dense(4096, activation=\"relu\"), nn.Dropout(0.5),\n",
    "        nn.Dense(4096, activation=\"relu\"), nn.Dropout(0.5),\n",
    "        # Output layer. Since we are using Fashion-MNIST, the number of\n",
    "        # classes is 10, instead of 1000 as in the paper\n",
    "        nn.Dense(num_outputs))\n",
    "    return net\n",
    "\n",
    "# construct and initialize network.\n",
    "ctx =  mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()\n",
    "\n",
    "net = construct_net()\n",
    "net.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "# define loss and trainer.\n",
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nd.random.uniform(shape=(1, 3, 224, 224))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_data.shape)\n",
    "resize1 = sample_data.reshape((-1,3, image_width,image_height))\n",
    "print(resize1.shape)\n",
    "# net(resize1)\n",
    "#net(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_batch(batch, ctx):\n",
    "    \"\"\"Return features and labels on ctx.\"\"\"\n",
    "    features, labels = batch\n",
    "    if labels.dtype != features.dtype:\n",
    "        labels = labels.astype(features.dtype)\n",
    "    return (gutils.split_and_load(features, ctx),\n",
    "            gutils.split_and_load(labels, ctx), features.shape[0])\n",
    "\n",
    "def evaluate_accuracy(data_iter, net, ctx=[mx.cpu()]):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    acc_sum, n = nd.array([0]), 0\n",
    "    for batch in data_iter:\n",
    "        features, labels, _ = _get_batch(batch, ctx)\n",
    "        for X, y in zip(features, labels):\n",
    "            y = y.astype('float32')\n",
    "            acc_sum += (net(X).argmax(axis=1) == y).sum().copyto(mx.cpu())\n",
    "            n += y.size\n",
    "        acc_sum.wait_to_read()\n",
    "    return acc_sum.asscalar() / n\n",
    "\n",
    "def train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,\n",
    "              num_epochs):\n",
    "    \"\"\"Train and evaluate a model with CPU or GPU.\"\"\"\n",
    "    print('training on', ctx)\n",
    "    loss = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
    "                 time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_ch5(net, train_iter, validation_iter, batch_size, trainer, ctx,\n",
    "              num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
