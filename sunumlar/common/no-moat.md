
# No Moat leaked Google paper May 4, 2023
## Google “We Have No Moat, And Neither Does OpenAI” 

- We have no secret sauce. Our best hope is to learn from and collaborate with what others are doing outside Google. We should prioritize enabling 3P integrations.

- People will not pay for a restricted model when free, unrestricted alternatives are comparable in quality. We should consider where our value add really is.

- Giant models are slowing us down. In the long run, the best models are the ones which can be iterated upon quickly. We should make small variants more than an afterthought, now that we know what is possible in the <20B parameter regime.

## Google “We Have No Moat, And Neither Does OpenAI” Key points

- Retraining models from scratch is the hard path
- Large models aren’t more capable in the long run if we can iterate faster on small models
- Data quality scales better than data size
- Directly Competing With Open Source Is a Losing Proposition
- Individuals are not constrained by licenses to the same degree as corporations
- Being your own customer means you understand the use case
- Owning the Ecosystem: Letting Open Source Work for Us

## Google “We Have No Moat, And Neither Does OpenAI” History

- Feb 24, 2023 – LLaMA is Launched
- March 12, 2023 – Language models on a Toaster
- March 13, 2023 – Fine Tuning on a Laptop
- March 28, 2023 – Open Source GPT-3
- April 3, 2023 – Real Humans Can’t Tell the Difference Between a 13B Open Model and ChatGPT
- April 15, 2023 – Open Source RLHF at ChatGPT Levels








