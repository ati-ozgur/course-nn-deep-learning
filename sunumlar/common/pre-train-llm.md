# pre-train LLM

## Dump language model 1 (Before pre-train)

{{< include ../figures/dump-language-model-1-en.mermaid >}}

- When LLM first start training, it randomly generates output.
- Therefore, probability of any word coming is $\frac{1}{vocabulary size}$
- GPT2 vocabulary size is 50257
- at the start of the pre-training, we expect to see similar number, 10.82, for loss value

## Dump language model 2 (Before pre-train)


{{< include ../figures/dump-language-model-together-en.mermaid >}}

- here, we have to different vocab sizes, 50257 and 100
- correspondingly, we will start with two different losses


## Training LLMs Example llama2

Training is like compression of the terabytes of text

![llama2 training](../images/training-Llama2.png)

## Pre-Train step 1 dataset

![](../images/pretrain-step1-dataset.png)

## Pre-Train step 2 tokenization

![](../images/pretrain-step2-tokenization.png)

## Pre-Train step 3 neural network training

![](../images/pretrain-step3-neural-network-training.png)

## Pre-Train step 4 inference

![](../images/pretrain-step4-inference.png)

## Datasets used to train GPT-3

![](../images/Datasets-used-to-train-GPT-3.png)
 
## Datasets LLama 2


- Our training corpus includes a new mix of data from publicly available sources, which does not include data
from Meta‚Äôs products or services. 
- We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. 
- We trained on **2 trillion tokens** of data as this
provides a good performance‚Äìcost trade-off, 
- up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.

## Copyrighted Works

- Meta staff torrented nearly 82TB of pirated books for AI training ‚Äî court records reveal copyright violations
- OpenAI has been sued by novelists as far back as June 2023 for using their books to train its large language models, 
- with The New York Times following suit in December. - Nvidia has also been on the receiving end of a lawsuit filed by writers for using 196,640 books to train its NeMo model, which has since been taken down. 
- A former Nvidia employee blew the whistle on the company in August of last year, saying that it scraped more than 426 thousand hours of videos daily for use in AI training. 
- More recently, OpenAI is investigating if DeepSeek illegally obtained data from ChatGPT, which just shows how ironic things can get.
- [source](https://www.tomshardware.com/tech-industry/artificial-intelligence/meta-staff-torrented-nearly-82tb-of-pirated-books-for-ai-training-court-records-reveal-copyright-violations)

## Using Copyrighted Works in LLMs (www.copyright.com)

- LLMs use massive amounts of textual works‚Äîmany of which are protected by copyright. 
- To do this, LLMs make copies of the works they rely on, which involves copyright in several ways, such as:

- Using copyright-protected material in the training datasets of LLMs without permission can result in the creation of unauthorized copies: copies generated during the training process and copies in the form of representations of the training data embedded within the LLM after training. This creates potential copyright liability.
- Outputs‚Äîthe material generated by AI systems like LLMs‚Äîmay create copyright liability if they are the same or too similar to one of the copyrighted works used as an input unless there is an appropriate copyright exception or limitation.
- [source](https://www.copyright.com/blog/heart-of-the-matter-copyright-ai-training-llms-executive-summary/)

## Datasets Hugging face FineWeb

- The üç∑ FineWeb dataset consists of more than 18.5T tokens (originally 15T tokens) of cleaned and deduplicated english web data from CommonCrawl. 
- The data processing pipeline is optimized for LLM performance and ran on the üè≠ datatrove library, our large scale data processing library. 

- https://huggingface.co/datasets/HuggingFaceFW/fineweb

## Pre training different models

![](../images/pre-training-different-models.png)

## Pre-training costs of different models

::: {style="font-size:14px"}

{{< include ../tables/table-pre-training-costs.md >}}

:::

## Sam-Altman-1million-GPU

![](../images/Sam-Altman-1million-GPU.png)

## pre-training speed up

- **We will not cover this topic**
- GPU parallelization (model, batch) 
- learning rates
- optimizers
- quantization (32 bit --> 16 bit)
- and others, **active research**


## pre-training tricks ( Let's reproduce GPT-2 (124M) Andrej Karpathy)

- **We will not cover this topic**
- Let‚Äôs make it fast. GPUs, mixed precision, 1000ms
- Tensor Cores, timing the code, TF32 precision, 333ms
- float16, gradient scalers, bfloat16, 300ms
- torch.compile, Python overhead, kernel fusion, 130ms
- flash attention, 96ms
- nice/ugly numbers. vocab size 50257 ‚Üí 50304, 93ms
- hyperpamaters, AdamW, gradient clipping
- learning rate scheduler: warmup + cosine decay
- batch size schedule, weight decay, FusedAdamW, 90ms
