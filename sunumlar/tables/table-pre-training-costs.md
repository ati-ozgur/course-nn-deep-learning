| Model                          | Release year | Reported training GPU-hours    | Reported GPU fleet / type           | Reported training time             | Estimated compute cost (USD)                        | Notes                                                                                   |
|--------------------------------|--------------|--------------------------------|-------------------------------------|------------------------------------|-----------------------------------------------------|-----------------------------------------------------------------------------------------|
| GPT-2 (1.5B) — original        | 2019         |                                | Not publicly disclosed              | Not publicly disclosed             | N/A                                                 | OpenAI did not publish training time or cost.                                           |
| GPT-2 (repro, modern hardware) | 2024         | ≈192 GPU-hrs (8×H100 for ~24h) | 8× H100 (80GB)                      | ≈24 hours                          | ≈$600–$800                                          | Community reproduction.  not the 2019 original run.                                     |
| GPT-3 (175B)                   | 2020         |                                | V100-era                            |  compute ≈3.14e23 FLOPs            | “Multiple weeks” (reports)                          | ≈$0.5M–$4.6M (est.) Cost varies widely by assumptions. compute from paper.              |
| GPT-4                          | 2023         |                                | Undisclosed (estimates: ~25k A100s) | Est. ~90–100 days (unofficial)     | > $100M (various estimates)                         | OpenAI hasn’t disclosed. Figures are outside estimates.                                 |
| Llama 2 (7B–70B)               | 2023         | ≈3.3M A100-80GB GPU-hrs        | NVIDIA A100 80GB                    | Depends on cluster size            | ≈$5M–$8M (at $1.5–$2.5/GPU-hr)                      | From Meta paper. cost uses common rental ranges.                                        |
| Llama 3.1 (405B)               | 2024         | ≈30.84M H100 GPU-hrs           | H100                                |  ~24k GPUs over ~54 days (reports) | ≈54 days (reported)                                 | ≈$62M–$93M (at $2–$3/GPU-hr). GPU-hrs from engineering report. time from news coverage. |
| Llama 4 Scout (17B)            | 2025         | ≈7.38M H100-80GB GPU-hrs       | H100 80GB                           | Not disclosed                      | ≈$15M–$22M (at $2–$3/GPU-hr)                        | From NVIDIA/Meta model card summary.                                                    |
| Claude 3.7 Sonnet              | 2025         |                                | Undisclosed (<1e26 FLOPs claim)     | Not disclosed                      | “Few tens of millions” (company guidance via press) | Anthropic hasn’t published full training details.                                       |
| Grok 2                         | 2024         |                                | ≈20,000 H100 (per Musk)             | Not disclosed                      | N/A (hours unknown)                                 | GPU count stated publicly.  no official hours.                                          |
| Grok 3                         | 2025         |                                | Claims: 100k–200k H100s             | Not disclosed                      | N/A (claims vary)                                   | Numbers are public claims/reports.  not independently verified.                         |
| DeepSeek-V3                    | 2024         | ≈2.788M H800 GPU-hrs           | NVIDIA H800                         | Depends on cluster size            | ≈$5.6M (@ $2/GPU-hr)                                | From DeepSeek paper and analyses.                                                       |