Lesson04.md

## Lesson 04 



04.01 Multi Layer Perceptron
04.01.01 Autograd
04.01.02 Datasets FashionMNIST and MNIST



04.02 Multi Layer Perceptron from scratch

04.03 Multi Layer Perceptron in Gluon


## Background Information

### Note 1:

Any linear function can be written in matrix product form.
See [an example here](https://www.khanacademy.org/math/linear-algebra/matrix-transformations/linear-transformations/v/linear-transformations-as-matrix-vector-products).

### Note 2:

Linear Regression can be solved using Least Squares approximation.
See [example video here](https://www.khanacademy.org/math/linear-algebra/alternate-bases/orthogonal-projections/v/linear-algebra-least-squares-approximation).

### Note 3:
Any function can be approximated with a MLP, the so called [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem).
See a [visual proof here](http://neuralnetworksanddeeplearning.com/chap4.html).





