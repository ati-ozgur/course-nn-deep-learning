# Yerleştirmeler (embeddings)

Problem, sayılaştırma (tokenization) sonucunda yazımızı sayılara dönüştürüyoruz ama bu sayılar arasında bir ilişkimiz yok.
Örneğin GPT2 transformer dog (köpek) kelimesini 9703, cat kelimesini 9246 ile sayılaştırmaktadır.
Aşağıdaki tabloda örnek olarak bazı GPT2 sayılaştırmaları verilmiştir.

{{< include ./tables/table-example-gpt2-tokenization-tr.md >}}

Biz kedi ve köpek'ler hakkında birçok bilgiye sahibiyiz.
Örneğin ikisininde 4 ayağı olduğunu, ev hayvanı olduğunu biliyoruz.
Yerleştirmelerin amacı aşağıdakine benzeyen bir şekilde bu kelimelerin sayılarına daha büyük bir uzaya dönüştürmektir.
Öğrenilen bu uzayda birbirine yakın kelimelerin (kedi, köpek) daha yakın olması amaçlanmaktadır.


{{< include ./tables/table-embedding-example-tr.md >}}



## Mühendislikte dönüşümler



{{< include ./figures/laplace-fourier-z-transforms.mermaid >}}

## Yerleştirme Oyun alanı

https://andkret.github.io/embedding-playground/



kelime yerleştirme için etkileşimli web uygulaması: [turbomaze.github.io/word2vecjson](https://turbomaze.github.io/word2vecjson/), [Kaynak kodu](https://github.com/turbomaze/word2vecjson).


Bu uygulama, word2vec'in 10.000 kelimelik bir alt kümesini kullanır. 
Kelimeleri ve [300 uzunluğundaki sayı dizisini]((https://turbomaze.github.io/word2vecjson/data/wordvecs10000.js)) inceleyin. 


## Kelime Yerleştirmeleri

- word2vec
- gensim




## Önceden Eğitilmiş Örnek Kelime Vektörleri: Gensim


- notebooks/Gensim_word_vector_visualization


## Kelime Vektörleri Eğitim Örneği

```python
{{< include ./src/word2vec_cbow_pytorch.py >}}
```

## Önceden eğitilmiş örnek 2: Sentence Transformer


```python
{{< include ./src/embeddings_sentence_transformer.py >}}
```


## Tavsiye Vidyolar

Başlıklar çevrilmemiştir.


- [What Are Word Embeddings?](https://www.youtube.com/watch?v=hVM8qGRTaOA)
- [https://www.youtube.com/watch?v=ArnMdc-ICCM](Embeddings: What they are and why they matter (Simon Willison))
