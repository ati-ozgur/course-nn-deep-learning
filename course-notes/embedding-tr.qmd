# Yerleştirmeler (embeddings)

## GPT mimarisinde yerleştirme

![GPT mimarisinde yerleştirme](./images/embeddings-in-GPT-architecture.png)








## Giriş

Problem, sayılaştırma (tokenization) sonucunda yazımızı sayılara dönüştürüyoruz ama bu sayılar arasında bir ilişkimiz yok.
Örneğin GPT2 transformer dog (köpek) kelimesini 9703, cat kelimesini 9246 ile sayılaştırmaktadır.
Aşağıdaki tabloda örnek olarak bazı GPT2 sayılaştırmaları verilmiştir.

{{< include ./tables/table-example-gpt2-tokenization-tr.md >}}


** Örnek: Mühendislik dönüşümleri **

Yerleştirmelerin mantığı mühendislikte kullanılan dönüştürmelere benzer.

{{< include ./figures/laplace-fourier-z-transforms.mermaid >}}

Biz 9703 (dog) ve 9246 (cat) ile matematiksel işlem tanımlamakta zorlanırız.
Bu sayılar ile oldukları gibi işlem yapmak yerine bu sayılar daha büyük bir yerleştirme uzayına dönüştürüp işlemleri orada yapıp, arkasından tekrar sözlük sayılaştırmalarına (tokenization) döndürmek daha kolay olacaktır.

** Örnek: 5 özellik **

Biz kedi ve köpek'ler hakkında birçok bilgiye sahibiyiz.
Örneğin ikisininde 4 ayağı olduğunu, ev hayvanı olduğunu biliyoruz.
Yerleştirmelerin amacı aşağıdakine benzeyen bir şekilde bu kelimelerin sayılarına daha büyük bir uzaya dönüştürmektir.

{{< include ./tables/table-embedding-example-tr.md >}}


Buradaki özellik boyutu 5 örnek olarak verilmiştir.
Yapay sinir ağları bu özellikleri kendisi öğrenir.
Normalde bu sayılar çok daha büyüktür.
Örneğin önceden eğitilmiş kelime yerleştirme Gensim glove-wiki-gigaword-100 modeli 100 boyutludur.

Öğrenilen bu uzayda birbirine yakın kelimelerin (kedi, köpek) daha yakın olması amaçlanmaktadır.


Gensim glove-wiki-gigaword-100 modeli wikipedia verisinde eğitilmiştir.
Bu modelin yerleştirmelerini PCA kullanarak 2 boyuta indirgeyerek görselleştirebiliriz.
Aşağıda bu işlemin örnek bir sonucunu görebilirsiniz.

![Gensim embeddings PCA](./images/gensim-embeddings-pca.png)

Yukarıdaki resimde cat ve dog kelimelerinin birbirine yakın olduğunu ve aynı zamanda alaklı kelimelerinde indirgendikleri 2 boyutta bile yakın olduklarına dikkat ediniz.


## Yerleştirme Boyutu ve LLM parametre sayısı

LLM modellerinin parametre sayısını belirleyen kıstaslardan biride yerleştirme boyutlarıdır.
Aşağıda bilinen bazı LLM modellerinin yerleştirme boyutları verilmiştir.

{{< include ./tables/table-embeddings-vs-llm-models-sizes.md >}}






## Yerleştirme Oyun alanı

https://andkret.github.io/embedding-playground/



kelime yerleştirme için etkileşimli web uygulaması: [turbomaze.github.io/word2vecjson](https://turbomaze.github.io/word2vecjson/), [Kaynak kodu](https://github.com/turbomaze/word2vecjson).


Bu uygulama, word2vec'in 10.000 kelimelik bir alt kümesini kullanır. 
Kelimeleri ve [300 uzunluğundaki sayı dizisini]((https://turbomaze.github.io/word2vecjson/data/wordvecs10000.js)) inceleyin. 


## Kelime Yerleştirmeleri

- word2vec
- gensim




## Önceden Eğitilmiş Örnek 1 Kelime Vektörleri: Gensim


- notebooks/Gensim_word_vector_visualization

## Önceden Eğitilmiş Örnek 2: Sentence Transformer


```python
{{< include ./src/embeddings_sentence_transformer.py >}}
```


## Kelime Vektörleri Eğitim Örneği


![](./images/word2vec-cbow-skip-gram.png)

[Kaynak word2vec makalesi](https://arxiv.org/pdf/1301.3781)




```python
{{< include ./src/word2vec_cbow_pytorch.py >}}
```

## Kitap Kodu



## Tavsiye Vidyolar

Başlıklar çevrilmemiştir.


- [What Are Word Embeddings?](https://www.youtube.com/watch?v=hVM8qGRTaOA)
- [https://www.youtube.com/watch?v=ArnMdc-ICCM](Embeddings: What they are and why they matter (Simon Willison))
