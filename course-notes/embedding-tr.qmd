# Yerleştirme (embedding)

Problem, sayılaştırma (tokenization) sonucunda yazımızı sayılara dönüştürüyoruz ama bu sayılar arasında ilişkimiz yok.
Örneğin GPT2 transformer dog (köpek) kelimesini 9703, cat kelimesini 9246 ile sayılaştırmaktadır.
Ama bu 

{{< include ./tables/table-example-gpt2-tokenization-tr.md >}}





## Mühendislikte dönüşümler

Complex numbers

The Laplace, Fourier and Z-Transforms
(Analogue)   (Frequence Domain) (Discrete)


Differential equation to algebraic equation

flowchart 
L--> F --> Z -->L 

https://andkret.github.io/embedding-playground/

{{< include ./tables/table-embedding-example-tr.md >}}


- word2vec
- gensim




kelime yerleştirme için etkileşimli web uygulaması: [turbomaze.github.io/word2vecjson](https://turbomaze.github.io/word2vecjson/), [Kaynak kodu](https://github.com/turbomaze/word2vecjson).


Bu uygulama, word2vec'in 10.000 kelimelik bir alt kümesini kullanır. 
Kelimeleri ve [300 uzunluğundaki sayı dizisini]((https://turbomaze.github.io/word2vecjson/data/wordvecs10000.js)) inceleyin. 

## Example with Sentence Transformer


```python
{{< include ./src/embeddings_sentence_transformer.py >}}
```



## Tavsiye Vidyolar

Başlıklar tercume edilmemiştir.


- [What Are Word Embeddings?](https://www.youtube.com/watch?v=hVM8qGRTaOA)
- [https://www.youtube.com/watch?v=ArnMdc-ICCM](Embeddings: What they are and why they matter (Simon Willison))
