# SayÄ±laÅŸtÄ±rma (Tokenization)

BiliÅŸim sÃ¶zlÃ¼ÄŸÃ¼ Ã§eviri olarak simgeleÅŸtirme vermiÅŸ ama bana gÃ¶re sayÄ±laÅŸtÄ±rma LLM baÄŸlamÄ±nda daha anlamlÄ±.


## NiÃ§in

TÃ¼m makine Ã¶ÄŸrenme yÃ¶ntemleri gibi, yapay sinir aÄŸlarÄ± da sadece sayÄ±lar ile Ã§alÄ±ÅŸtÄ±ÄŸÄ± iÃ§in verilen metinleri sayÄ±lara Ã§evirmek gerekiyor.


{{< include ./figures/tokenize-workflow.mermaid >}}

## GÃ¶rselleÅŸtirme 


- [Ã§evrimiÃ§i tiktokenizer](https://tiktokenizer.vercel.app/?model=gpt2)

![](../images/tiktokenizer.vercel.app.png)

![](../images/tiktokenizer.vercel.app-example-tr.png)


- [sadece gpt2 gÃ¶rselleÅŸtirme aracÄ±](https://github.com/sinjoysaha/tiktokenizer-js) kodu aÃ§Ä±k ve internet olmadan Ã§alÄ±ÅŸtÄ±rÄ±labiliyor.


### Ã–rnek simgeleÅŸtirme yazÄ±sÄ±

```txt
{{< include ./src/tokenization-example-string-Andrej-Karpathy.txt >}}
```

## Neden Ã¶nemli?

Andrej Karpathy 2 saatlik [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE) sunumunda aÅŸaÄŸÄ±daki cÃ¼mle ile baÅŸlamaktadÄ±r ve arkasÄ±ndan aÅŸaÄŸÄ±daki tablo verilmektedir. 


	SayÄ±laÅŸtÄ±rma (Tokenization), LLM'lerin birÃ§ok tuhaflÄ±ÄŸÄ±nÄ±n merkezinde yer alÄ±r. 
	Bunu gÃ¶z ardÄ± etmeyin.


{{< include ./tables/table-tokenization-karpathy-tr.md >}}


## SayÄ±laÅŸtÄ±rma TÃ¼rleri

1. Karakter tabanlÄ±

	- unicode
	- n-grams (karakter tabanlÄ± 1-n grams)

2. Kelime TabanlÄ±

	- Kelimeden tamsayÄ±lara (Word to integers). Ã–rnek kod: SimpleTokenizerV1
	- Bag of words (uni-gram word based)
	- one-hot-encoding

3. alt kelime (hece benzeri) tabanlÄ±

	- Byte pair encoding (GPT versions)
	- Sentence piece (LLama versions)

SÃ¶zlÃ¼k bÃ¼yÃ¼k olduÄŸunda, kodlanmÄ±ÅŸ sayÄ±lar daha kÄ±sa olur.
Ã‡Ã¼nkÃ¼ Ã§oÄŸu sÃ¶zlÃ¼k, en Ã§ok kullanÄ±lan kelimeleri tek bir sayÄ±ya kodlamak iÃ§in frekans analizi kullanÄ±r.
Bu durumda, LLM parametre sayÄ±sÄ± daha az olacaktÄ±r.


SÃ¶zlÃ¼k kÃ¼Ã§Ã¼k olduÄŸunda, girdi yazÄ±sÄ± daha uzun bir sayÄ± kÃ¼mesi ile kodlanacaktÄ±r.
LLM parametre sayÄ±sÄ± azalÄ±r ama bir sonraki sayÄ± (token) tahmini zorlaÅŸÄ±r.


### Unicode kendisi kullanma

- SÃ¶zlÃ¼k geniÅŸ
- Unicode standardÄ± canlÄ± ve deÄŸiÅŸiyor. 
- Bu durumda boyutlarÄ± deÄŸiÅŸiyor.
- Yeni gelen Unicode sembollerde sorun yaÅŸanacaktÄ±r.

Ã–rneÄŸin [Anadolu hiyeroglifleri](https://en.wikipedia.org/wiki/Anatolian_Hieroglyphs_(Unicode_block)), [emojiler](https://www.unicode.org/emoji/charts/full-emoji-list.html) ...  eklendi

- ğ”€â€ 	ğ”â€ 	ğ”‚â€ 	ğ”ƒâ€ 	ğ”„â€ 	ğ”…
- ğŸ‘€ 	ğŸ‘ï¸ 	ğŸ‘‚ 	ğŸ‘ƒ 	ğŸ‘„ 	ğŸ‘… 	ğŸ‘† 	ğŸ‘‡ 	ğŸ‘ˆ 	ğŸ‘‰ 	ğŸ‘Š 	ğŸ‘‹ 	ğŸ‘Œ 	ğŸ‘ 	ğŸ‘ 	ğŸ‘

utf8,utf16,utf32 gibi farklÄ± standartlar var.

- [unicode wikipedia](https://en.wikipedia.org/wiki/Unicode)
SÃ¼rÃ¼m 16.0, Ã§eÅŸitli sÄ±radan, edebi, akademik ve teknik baÄŸlamlarda kullanÄ±lan 154.998 karakteri ve 168 betiÄŸi tanÄ±mlar.



### Sorunlar tekrar

2a. talk number issue
2b. upper lowercase
2c. Non English languages
2d. Code
2e. Another tokenizer cl100_base
this works better for python code



127 + 677 = 804
1275 + 6773 = 8041

.DefaultCellStyle
SolidGoldMagikarp 
single reddit user.
Tokenization dataset and LLM training dataset is different.
SolidGoldMagikarp this token does not exists in LLM training dataset.
like Un-allocated memory in C


- [SolidGoldMagikarp article](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)




## Tokenization Vocabulary Sizes Of LLM Models

{{< include ./tables/table-tokenization-vocabulary-sizes-of-llm-models.md >}}



## Ã‡ift bayt kodlama (Byte pair encoding)

https://en.wikipedia.org/wiki/Byte-pair_encoding
https://www.geeksforgeeks.org/nlp/byte-pair-encoding-bpe-in-nlp/

### Byte pair encoding: NasÄ±l Ã§alÄ±ÅŸÄ±r

https://github.com/karpathy/minbpe/blob/master/exercise.md

## Kaynak Vidyolar

[Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)

## BaÄŸlantÄ±lar

https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/
https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation
https://www.lesswrong.com/posts/Ya9LzwEbfaAMY8ABo/solidgoldmagikarp-ii-technical-details-and-more-recent
https://aizi.substack.com/p/explaining-solidgoldmagikarp-by-looking
https://deconstructing.ai/deconstructing-ai%E2%84%A2-blog/f/the-enigma-of-solidgoldmagikarp-ais-strangest-token
## Other Libraries

https://github.com/google/sentencepiece
https://github.com/openai/tiktoken

https://github.com/karpathy/minbpe/

