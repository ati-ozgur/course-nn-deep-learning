# Sayılaştırma (Tokenization)

Bilişim sözlüğü çeviri olarak simgeleştirme vermiş ama bana göre sayılaştırma LLM bağlamında daha anlamlı.


## Niçin

Tüm makine öğrenme yöntemleri gibi, yapay sinir ağları da sadece sayılar ile çalıştığı için verilen metinleri sayılara çevirmek gerekiyor.


{{< include ./figures/tokenize-workflow.mermaid >}}

## Görselleştirme 


- [çevrimiçi tiktokenizer](https://tiktokenizer.vercel.app/?model=gpt2)

![](../images/tiktokenizer.vercel.app.png)

![](../images/tiktokenizer.vercel.app-example-tr.png)


- [sadece gpt2 görselleştirme aracı](https://github.com/sinjoysaha/tiktokenizer-js) kodu açık ve internet olmadan çalıştırılabiliyor.


### Örnek simgeleştirme yazısı

```txt
{{< include ./src/tokenization-example-string-Andrej-Karpathy.txt >}}
```

## Neden önemli?

Andrej Karpathy 2 saatlik [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE) sunumunda aşağıdaki cümle ile başlamaktadır ve arkasından aşağıdaki tablo verilmektedir. 


	Sayılaştırma (Tokenization), LLM'lerin birçok tuhaflığının merkezinde yer alır. 
	Bunu göz ardı etmeyin.


{{< include ./tables/table-tokenization-karpathy-tr.md >}}


## Sayılaştırma Türleri


{{< include ./figures/tokenization-types-en.mermaid >}}


1. Karakter tabanlı

	- unicode
	- n-grams (karakter tabanlı 1-n grams)

2. Kelime Tabanlı

	- Kelimeden tamsayılara (Word to integers). Örnek kod: SimpleTokenizerV1
	- Bag of words (uni-gram word based)
	- one-hot-encoding

3. alt kelime (hece benzeri) tabanlı

	- Byte pair encoding (GPT versions)
	- Sentence piece (LLama versions)

Sözlük büyük olduğunda, kodlanmış sayılar daha kısa olur.
Çünkü çoğu sözlük, en çok kullanılan kelimeleri tek bir sayıya kodlamak için frekans analizi kullanır.
Bu durumda, LLM parametre sayısı daha az olacaktır.


Sözlük küçük olduğunda, girdi yazısı daha uzun bir sayı kümesi ile kodlanacaktır.
LLM parametre sayısı azalır ama bir sonraki sayı (token) tahmini zorlaşır.


### Unicode kendisi kullanma

- Sözlük geniş
- Unicode standardı canlı ve değişiyor. 
- Bu durumda boyutları değişiyor.
- Yeni gelen Unicode sembollerde sorun yaşanacaktır.

Örneğin [Anadolu hiyeroglifleri](https://en.wikipedia.org/wiki/Anatolian_Hieroglyphs_(Unicode_block)), [emojiler](https://www.unicode.org/emoji/charts/full-emoji-list.html) ...  eklendi

- 𔐀‎ 	𔐁‎ 	𔐂‎ 	𔐃‎ 	𔐄‎ 	𔐅
- 👀 	👁️ 	👂 	👃 	👄 	👅 	👆 	👇 	👈 	👉 	👊 	👋 	👌 	👍 	👎 	👏

utf8,utf16,utf32 gibi farklı standartlar var.

- [unicode wikipedia](https://en.wikipedia.org/wiki/Unicode)
Sürüm 16.0, çeşitli sıradan, edebi, akademik ve teknik bağlamlarda kullanılan 154.998 karakteri ve 168 betiği tanımlar.



### Sorunlar tekrar

- Heceleme

```txt
strawberry
heceleme
```


- Aritmetik, toplama çıkarma

```txt
127 + 677 = 804
1275 + 6773 = 8041
```


- Büyük küçük harf

```txt
hello
 hello
HELLO
```


- İngilizce harici diller

```txt
Merhaba Büyük Dil Modelleri dersine giriş
```

- Python kodlama problemi: GPT2 vs cl100_base


```python
for i in range(1, 101):
    if i % 3 == 0 and i % 5 == 0:
        print("Fifteen")
    elif i % 3 == 0:
        print("Three")
    elif i % 5 == 0:
        print("Five")
    else:
        print(i)
```


- JSON vs YAML

JSON örnek:

```json
 {"name":"John", "age":30, "car":null}
```

YAML versiyonu:


```yaml
---
name: John
age: 30
car: 
```

### SolidGoldMagikarp 

Bu bir reddit kullanıcısı.
ChatGPT 14 Şubat 2023'ten önce **SolidGoldMagikarp** hakkında soru sorulduğu zaman hata veriyordu.
GPT3 için BPE algoritması eğitilirken reddit üzerinden alınan bir metin grubu kullanılmış.
SolidGoldMagikarp bu yazı grubu içinde çok fazla yazının sahibi.

Tahminlere göre, Sayılaştırma veri seti ile LLM veri seti farklı.
SolidGoldMagikarp sayısı (token), LLM veri setinde yok.
Bu yüzden girdi olarak verildiğinde C dilinde oluşan atanmamış bellek (Un-allocated memory) hatası gibi bir şey oluyor.
Daha fazlası için bakınız: 

- [SolidGoldMagikarp article](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)
- https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/
- https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation
- https://www.lesswrong.com/posts/Ya9LzwEbfaAMY8ABo/solidgoldmagikarp-ii-technical-details-and-more-recent
- https://aizi.substack.com/p/explaining-solidgoldmagikarp-by-looking
- https://deconstructing.ai/deconstructing-ai%E2%84%A2-blog/f/the-enigma-of-solidgoldmagikarp-ais-strangest-token



## LLM Modellerinin Tokenizasyon Sözlük Boyutları 

ChatGPT 5 ile üretilmiştir.

{{< include ./tables/table-tokenization-vocabulary-sizes-of-llm-models-en.md >}}



## Çift bayt kodlama (Byte pair encoding)

https://en.wikipedia.org/wiki/Byte-pair_encoding
https://www.geeksforgeeks.org/nlp/byte-pair-encoding-bpe-in-nlp/

### Byte pair encoding: Nasıl çalışır

https://github.com/karpathy/minbpe/blob/master/exercise.md

## Kaynak Vidyolar

[Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)


## Kütüphaneler

- https://github.com/google/sentencepiece
- https://github.com/openai/tiktoken
- https://github.com/karpathy/minbpe/

