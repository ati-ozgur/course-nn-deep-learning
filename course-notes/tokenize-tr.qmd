# Simgeleştirme (Tokenization)

Bilişim sözlüğü çeviri olarak simgeleştirme vermiş ama bana göre birimleştirme LLM bağlamında daha anlamlı.


## Niçin

Tüm makine öğrenme yöntemleri gibi, yapay sinir ağları da sadece sayılar ile çalıştığı için verilen metinleri sayılara çevirmek gerekiyor.



## Neden önemli?

Andrej Karpathy sunumunda

	Simgeleştirme (Tokenization) :(

	Tokenleştirme, LLM'lerin birçok tuhaflığının merkezinde yer alır. 
	Bunu göz ardı etmeyin.


{{< include ./tables/table-tokenization-karpathy.md >}}


## Simgeleştirme Türleri

1. character based

	- just unicode
	- n-grams (character based 1-n grams)

2. word-based

	- Word to integers (SimpleTokenizerV1)
	- Bag of words (uni-gram word based)
	- one-hot-encoding

3. sub-word based

	- Byte pair encoding (GPT versions)
	- Sentence piece (LLama versions)

Sözlük büyük olduğunda, kodlanmış sayılar daha kısa olur.
Çünkü çoğu sözlük, en çok kullanılan kelimeleri tek bir sayıya kodlamak için frekans analizi kullanır.
Sözlük küçük olduğunda, tüm kodlanmış sayılar daha uzun olacak ve diziler uzun olacaktır.

### Use Unicode itself

Sözlük geniş
Unicode standardı canlı ve değişiyor.
Bu durumda boyutları değişiyor.
Örneğin [Anadolu hiyeroglifleri](https://en.wikipedia.org/wiki/Anatolian_Hieroglyphs_(Unicode_block)), [emojiler](https://www.unicode.org/emoji/charts/full-emoji-list.html) ...  eklendi

- 𔐀‎ 	𔐁‎ 	𔐂‎ 	𔐃‎ 	𔐄‎ 	𔐅
- 👀 	👁️ 	👂 	👃 	👄 	👅 	👆 	👇 	👈 	👉 	👊 	👋 	👌 	👍 	👎 	👏

utf8,utf16,utf32 gibi farklı standartlar var.

- [unicode wikipedia](https://en.wikipedia.org/wiki/Unicode)
Sürüm 16.0, çeşitli sıradan, edebi, akademik ve teknik bağlamlarda kullanılan 154.998 karakteri ve 168 betiği tanımlar.







## Visualizer

- [online visualizar tool](https://tiktokenizer.vercel.app/?model=gpt2)
- [visualizer for gpt2 only](https://github.com/sinjoysaha/tiktokenizer-js)

yerel kod var.


### Örnek resimler

![](../images/tiktokenizer.vercel.app.png)

![](../images/tiktokenizer.vercel.app-example-tr.png)

### Örnek simgeleştirme yazısı

```txt
{{< include ./src/tokenization-example-string-Andrej-Karpathy.txt >}}
```

### Sorunlar tekrar

2a. talk number issue
2b. upper lowercase
2c. Non English languages
2d. Code
2e. Another tokenizer cl100_base
this works better for python code



127 + 677 = 804
1275 + 6773 = 8041

.DefaultCellStyle
SolidGoldMagikarp 
single reddit user.
Tokenization dataset and LLM training dataset is different.
SolidGoldMagikarp this token does not exists in LLM training dataset.
like Un-allocated memory in C


- [SolidGoldMagikarp article](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)




## Tokenization Vocabulary Sizes Of Llm Models

{{< include ./tables/table-tokenization-vocabulary-sizes-of-llm-models.md >}}



## Byte pair encoding

https://en.wikipedia.org/wiki/Byte-pair_encoding
https://www.geeksforgeeks.org/nlp/byte-pair-encoding-bpe-in-nlp/

### Byte pair encoding: How it works

https://github.com/karpathy/minbpe/blob/master/exercise.md

## Videos

[Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)

## Links

https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/
https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation
https://www.lesswrong.com/posts/Ya9LzwEbfaAMY8ABo/solidgoldmagikarp-ii-technical-details-and-more-recent
https://aizi.substack.com/p/explaining-solidgoldmagikarp-by-looking
https://deconstructing.ai/deconstructing-ai%E2%84%A2-blog/f/the-enigma-of-solidgoldmagikarp-ais-strangest-token
## Other Libraries

https://github.com/google/sentencepiece
https://github.com/openai/tiktoken

https://github.com/karpathy/minbpe/

