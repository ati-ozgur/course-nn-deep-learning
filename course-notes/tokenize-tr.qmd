# Sayılaştırma (Tokenization)

Bilişim sözlüğü çeviri olarak simgeleştirme vermiş ama bana göre sayılaştırma LLM bağlamında daha anlamlı.


## Niçin

Tüm makine öğrenme yöntemleri gibi, yapay sinir ağları da sadece sayılar ile çalıştığı için verilen metinleri sayılara çevirmek gerekiyor.


{{< include ./figures/tokenize-workflow.mermaid >}}

## Görselleştirme 


- [çevrimiçi tiktokenizer](https://tiktokenizer.vercel.app/?model=gpt2)

![](../images/tiktokenizer.vercel.app.png)

![](../images/tiktokenizer.vercel.app-example-tr.png)


- [sadece gpt2 görselleştirme aracı](https://github.com/sinjoysaha/tiktokenizer-js) kodu açık ve internet olmadan çalıştırılabiliyor.


### Örnek simgeleştirme yazısı

```txt
{{< include ./src/tokenization-example-string-Andrej-Karpathy.txt >}}
```

## Neden önemli?

Andrej Karpathy 2 saatlik [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE) sunumunda aşağıdaki cümle ile başlamaktadır ve arkasından aşağıdaki tablo verilmektedir. 


	Sayılaştırma (Tokenization), LLM'lerin birçok tuhaflığının merkezinde yer alır. 
	Bunu göz ardı etmeyin.


{{< include ./tables/table-tokenization-karpathy-tr.md >}}


## Sayılaştırma Türleri

1. Karakter tabanlı

	- unicode
	- n-grams (karakter tabanlı 1-n grams)

2. Kelime Tabanlı

	- Kelimeden tamsayılara (Word to integers). Örnek kod: SimpleTokenizerV1
	- Bag of words (uni-gram word based)
	- one-hot-encoding

3. alt kelime (hece benzeri) tabanlı

	- Byte pair encoding (GPT versions)
	- Sentence piece (LLama versions)

Sözlük büyük olduğunda, kodlanmış sayılar daha kısa olur.
Çünkü çoğu sözlük, en çok kullanılan kelimeleri tek bir sayıya kodlamak için frekans analizi kullanır.
Bu durumda, LLM parametre sayısı daha az olacaktır.


Sözlük küçük olduğunda, girdi yazısı daha uzun bir sayı kümesi ile kodlanacaktır.
LLM parametre sayısı azalır ama bir sonraki sayı (token) tahmini zorlaşır.


### Unicode kendisi kullanma

- Sözlük geniş
- Unicode standardı canlı ve değişiyor. 
- Bu durumda boyutları değişiyor.
- Yeni gelen Unicode sembollerde sorun yaşanacaktır.

Örneğin [Anadolu hiyeroglifleri](https://en.wikipedia.org/wiki/Anatolian_Hieroglyphs_(Unicode_block)), [emojiler](https://www.unicode.org/emoji/charts/full-emoji-list.html) ...  eklendi

- 𔐀‎ 	𔐁‎ 	𔐂‎ 	𔐃‎ 	𔐄‎ 	𔐅
- 👀 	👁️ 	👂 	👃 	👄 	👅 	👆 	👇 	👈 	👉 	👊 	👋 	👌 	👍 	👎 	👏

utf8,utf16,utf32 gibi farklı standartlar var.

- [unicode wikipedia](https://en.wikipedia.org/wiki/Unicode)
Sürüm 16.0, çeşitli sıradan, edebi, akademik ve teknik bağlamlarda kullanılan 154.998 karakteri ve 168 betiği tanımlar.



### Sorunlar tekrar

2a. talk number issue
2b. upper lowercase
2c. Non English languages
2d. Code
2e. Another tokenizer cl100_base
this works better for python code



127 + 677 = 804
1275 + 6773 = 8041

.DefaultCellStyle
SolidGoldMagikarp 
single reddit user.
Tokenization dataset and LLM training dataset is different.
SolidGoldMagikarp this token does not exists in LLM training dataset.
like Un-allocated memory in C


- [SolidGoldMagikarp article](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)




## Tokenization Vocabulary Sizes Of LLM Models

{{< include ./tables/table-tokenization-vocabulary-sizes-of-llm-models.md >}}



## Çift bayt kodlama (Byte pair encoding)

https://en.wikipedia.org/wiki/Byte-pair_encoding
https://www.geeksforgeeks.org/nlp/byte-pair-encoding-bpe-in-nlp/

### Byte pair encoding: Nasıl çalışır

https://github.com/karpathy/minbpe/blob/master/exercise.md

## Kaynak Vidyolar

[Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)

## Bağlantılar

https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/
https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation
https://www.lesswrong.com/posts/Ya9LzwEbfaAMY8ABo/solidgoldmagikarp-ii-technical-details-and-more-recent
https://aizi.substack.com/p/explaining-solidgoldmagikarp-by-looking
https://deconstructing.ai/deconstructing-ai%E2%84%A2-blog/f/the-enigma-of-solidgoldmagikarp-ais-strangest-token
## Other Libraries

https://github.com/google/sentencepiece
https://github.com/openai/tiktoken

https://github.com/karpathy/minbpe/

