# SimgeleÅŸtirme (Tokenization)

BiliÅŸim sÃ¶zlÃ¼ÄŸÃ¼ Ã§eviri olarak simgeleÅŸtirme vermiÅŸ ama bana gÃ¶re birimleÅŸtirme LLM baÄŸlamÄ±nda daha anlamlÄ±.


## NiÃ§in

TÃ¼m makine Ã¶ÄŸrenme yÃ¶ntemleri gibi, yapay sinir aÄŸlarÄ± da sadece sayÄ±lar ile Ã§alÄ±ÅŸtÄ±ÄŸÄ± iÃ§in verilen metinleri sayÄ±lara Ã§evirmek gerekiyor.



## Neden Ã¶nemli?

Andrej Karpathy sunumunda

	SimgeleÅŸtirme (Tokenization) :(

	TokenleÅŸtirme, LLM'lerin birÃ§ok tuhaflÄ±ÄŸÄ±nÄ±n merkezinde yer alÄ±r. 
	Bunu gÃ¶z ardÄ± etmeyin.


{{< include ./tables/table-tokenization-karpathy.md >}}


## SimgeleÅŸtirme TÃ¼rleri

1. character based

	- just unicode
	- n-grams (character based 1-n grams)

2. word-based

	- Word to integers (SimpleTokenizerV1)
	- Bag of words (uni-gram word based)
	- one-hot-encoding

3. sub-word based

	- Byte pair encoding (GPT versions)
	- Sentence piece (LLama versions)

SÃ¶zlÃ¼k bÃ¼yÃ¼k olduÄŸunda, kodlanmÄ±ÅŸ sayÄ±lar daha kÄ±sa olur.
Ã‡Ã¼nkÃ¼ Ã§oÄŸu sÃ¶zlÃ¼k, en Ã§ok kullanÄ±lan kelimeleri tek bir sayÄ±ya kodlamak iÃ§in frekans analizi kullanÄ±r.
SÃ¶zlÃ¼k kÃ¼Ã§Ã¼k olduÄŸunda, tÃ¼m kodlanmÄ±ÅŸ sayÄ±lar daha uzun olacak ve diziler uzun olacaktÄ±r.

### Use Unicode itself

SÃ¶zlÃ¼k geniÅŸ
Unicode standardÄ± canlÄ± ve deÄŸiÅŸiyor.
Bu durumda boyutlarÄ± deÄŸiÅŸiyor.
Ã–rneÄŸin [Anadolu hiyeroglifleri](https://en.wikipedia.org/wiki/Anatolian_Hieroglyphs_(Unicode_block)), [emojiler](https://www.unicode.org/emoji/charts/full-emoji-list.html) ...  eklendi

- ğ”€â€ 	ğ”â€ 	ğ”‚â€ 	ğ”ƒâ€ 	ğ”„â€ 	ğ”…
- ğŸ‘€ 	ğŸ‘ï¸ 	ğŸ‘‚ 	ğŸ‘ƒ 	ğŸ‘„ 	ğŸ‘… 	ğŸ‘† 	ğŸ‘‡ 	ğŸ‘ˆ 	ğŸ‘‰ 	ğŸ‘Š 	ğŸ‘‹ 	ğŸ‘Œ 	ğŸ‘ 	ğŸ‘ 	ğŸ‘

utf8,utf16,utf32 gibi farklÄ± standartlar var.

- [unicode wikipedia](https://en.wikipedia.org/wiki/Unicode)
SÃ¼rÃ¼m 16.0, Ã§eÅŸitli sÄ±radan, edebi, akademik ve teknik baÄŸlamlarda kullanÄ±lan 154.998 karakteri ve 168 betiÄŸi tanÄ±mlar.







## Visualizer

- [online visualizar tool](https://tiktokenizer.vercel.app/?model=gpt2)
- [visualizer for gpt2 only](https://github.com/sinjoysaha/tiktokenizer-js)

yerel kod var.


### Ã–rnek resimler

![](../images/tiktokenizer.vercel.app.png)

![](../images/tiktokenizer.vercel.app-example-tr.png)

### Ã–rnek simgeleÅŸtirme yazÄ±sÄ±

```txt
{{< include ./src/tokenization-example-string-Andrej-Karpathy.txt >}}
```

### Sorunlar tekrar

2a. talk number issue
2b. upper lowercase
2c. Non English languages
2d. Code
2e. Another tokenizer cl100_base
this works better for python code



127 + 677 = 804
1275 + 6773 = 8041

.DefaultCellStyle
SolidGoldMagikarp 
single reddit user.
Tokenization dataset and LLM training dataset is different.
SolidGoldMagikarp this token does not exists in LLM training dataset.
like Un-allocated memory in C


- [SolidGoldMagikarp article](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)




## Tokenization Vocabulary Sizes Of Llm Models

{{< include ./tables/table-tokenization-vocabulary-sizes-of-llm-models.md >}}



## Byte pair encoding

https://en.wikipedia.org/wiki/Byte-pair_encoding
https://www.geeksforgeeks.org/nlp/byte-pair-encoding-bpe-in-nlp/

### Byte pair encoding: How it works

https://github.com/karpathy/minbpe/blob/master/exercise.md

## Videos

[Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)

## Links

https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/
https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation
https://www.lesswrong.com/posts/Ya9LzwEbfaAMY8ABo/solidgoldmagikarp-ii-technical-details-and-more-recent
https://aizi.substack.com/p/explaining-solidgoldmagikarp-by-looking
https://deconstructing.ai/deconstructing-ai%E2%84%A2-blog/f/the-enigma-of-solidgoldmagikarp-ais-strangest-token
## Other Libraries

https://github.com/google/sentencepiece
https://github.com/openai/tiktoken

https://github.com/karpathy/minbpe/

