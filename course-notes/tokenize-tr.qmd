# SayÄ±laÅŸtÄ±rma (Tokenization)

BiliÅŸim sÃ¶zlÃ¼ÄŸÃ¼ Ã§eviri olarak simgeleÅŸtirme vermiÅŸ ama bana gÃ¶re sayÄ±laÅŸtÄ±rma LLM baÄŸlamÄ±nda daha anlamlÄ±.


## NiÃ§in

TÃ¼m makine Ã¶ÄŸrenme yÃ¶ntemleri gibi, yapay sinir aÄŸlarÄ± da sadece sayÄ±lar ile Ã§alÄ±ÅŸtÄ±ÄŸÄ± iÃ§in verilen metinleri sayÄ±lara Ã§evirmek gerekiyor.


{{< include ./figures/tokenize-workflow.mermaid >}}

## GÃ¶rselleÅŸtirme 


- [Ã§evrimiÃ§i tiktokenizer](https://tiktokenizer.vercel.app/?model=gpt2)

![](../images/tiktokenizer.vercel.app.png)

![](../images/tiktokenizer.vercel.app-example-tr.png)


- [sadece gpt2 gÃ¶rselleÅŸtirme aracÄ±](https://github.com/sinjoysaha/tiktokenizer-js) kodu aÃ§Ä±k ve internet olmadan Ã§alÄ±ÅŸtÄ±rÄ±labiliyor.


### Ã–rnek simgeleÅŸtirme yazÄ±sÄ±

```txt
{{< include ./src/tokenization-example-string-Andrej-Karpathy.txt >}}
```

## Neden Ã¶nemli?

Andrej Karpathy 2 saatlik [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE) sunumunda aÅŸaÄŸÄ±daki cÃ¼mle ile baÅŸlamaktadÄ±r ve arkasÄ±ndan aÅŸaÄŸÄ±daki tablo verilmektedir. 


	SayÄ±laÅŸtÄ±rma (Tokenization), LLM'lerin birÃ§ok tuhaflÄ±ÄŸÄ±nÄ±n merkezinde yer alÄ±r. 
	Bunu gÃ¶z ardÄ± etmeyin.


{{< include ./tables/table-tokenization-karpathy-tr.md >}}


## SayÄ±laÅŸtÄ±rma TÃ¼rleri


{{< include ./figures/tokenization-types-en.mermaid >}}


1. Karakter tabanlÄ±

	- unicode
	- n-grams (karakter tabanlÄ± 1-n grams)

2. Kelime TabanlÄ±

	- Kelimeden tamsayÄ±lara (Word to integers). Ã–rnek kod: SimpleTokenizerV1
	- Bag of words (uni-gram word based)
	- one-hot-encoding

3. alt kelime (hece benzeri) tabanlÄ±

	- Byte pair encoding (GPT versions)
	- Sentence piece (LLama versions)

SÃ¶zlÃ¼k bÃ¼yÃ¼k olduÄŸunda, kodlanmÄ±ÅŸ sayÄ±lar daha kÄ±sa olur.
Ã‡Ã¼nkÃ¼ Ã§oÄŸu sÃ¶zlÃ¼k, en Ã§ok kullanÄ±lan kelimeleri tek bir sayÄ±ya kodlamak iÃ§in frekans analizi kullanÄ±r.
Bu durumda, LLM parametre sayÄ±sÄ± daha az olacaktÄ±r.


SÃ¶zlÃ¼k kÃ¼Ã§Ã¼k olduÄŸunda, girdi yazÄ±sÄ± daha uzun bir sayÄ± kÃ¼mesi ile kodlanacaktÄ±r.
LLM parametre sayÄ±sÄ± azalÄ±r ama bir sonraki sayÄ± (token) tahmini zorlaÅŸÄ±r.


### Unicode kendisi kullanma

- SÃ¶zlÃ¼k geniÅŸ
- Unicode standardÄ± canlÄ± ve deÄŸiÅŸiyor. 
- Bu durumda boyutlarÄ± deÄŸiÅŸiyor.
- Yeni gelen Unicode sembollerde sorun yaÅŸanacaktÄ±r.

Ã–rneÄŸin [Anadolu hiyeroglifleri](https://en.wikipedia.org/wiki/Anatolian_Hieroglyphs_(Unicode_block)), [emojiler](https://www.unicode.org/emoji/charts/full-emoji-list.html) ...  eklendi

- ğ”€â€ 	ğ”â€ 	ğ”‚â€ 	ğ”ƒâ€ 	ğ”„â€ 	ğ”…
- ğŸ‘€ 	ğŸ‘ï¸ 	ğŸ‘‚ 	ğŸ‘ƒ 	ğŸ‘„ 	ğŸ‘… 	ğŸ‘† 	ğŸ‘‡ 	ğŸ‘ˆ 	ğŸ‘‰ 	ğŸ‘Š 	ğŸ‘‹ 	ğŸ‘Œ 	ğŸ‘ 	ğŸ‘ 	ğŸ‘

utf8,utf16,utf32 gibi farklÄ± standartlar var.

- [unicode wikipedia](https://en.wikipedia.org/wiki/Unicode)
SÃ¼rÃ¼m 16.0, Ã§eÅŸitli sÄ±radan, edebi, akademik ve teknik baÄŸlamlarda kullanÄ±lan 154.998 karakteri ve 168 betiÄŸi tanÄ±mlar.



### Sorunlar tekrar

- Heceleme

```txt
strawberry
heceleme
```


- Aritmetik, toplama Ã§Ä±karma

```txt
127 + 677 = 804
1275 + 6773 = 8041
```


- BÃ¼yÃ¼k kÃ¼Ã§Ã¼k harf

```txt
hello
 hello
HELLO
```


- Ä°ngilizce harici diller

```txt
Merhaba BÃ¼yÃ¼k Dil Modelleri dersine giriÅŸ
```

- Python kodlama problemi: GPT2 vs cl100_base


```python
for i in range(1, 101):
    if i % 3 == 0 and i % 5 == 0:
        print("Fifteen")
    elif i % 3 == 0:
        print("Three")
    elif i % 5 == 0:
        print("Five")
    else:
        print(i)
```


- JSON vs YAML

JSON Ã¶rnek:

```json
 {"name":"John", "age":30, "car":null}
```

YAML versiyonu:


```yaml
---
name: John
age: 30
car: 
```

### SolidGoldMagikarp 

Bu bir reddit kullanÄ±cÄ±sÄ±.
ChatGPT 14 Åubat 2023'ten Ã¶nce **SolidGoldMagikarp** hakkÄ±nda soru sorulduÄŸu zaman hata veriyordu.
GPT3 iÃ§in BPE algoritmasÄ± eÄŸitilirken reddit Ã¼zerinden alÄ±nan bir metin grubu kullanÄ±lmÄ±ÅŸ.
SolidGoldMagikarp bu yazÄ± grubu iÃ§inde Ã§ok fazla yazÄ±nÄ±n sahibi.

Tahminlere gÃ¶re, SayÄ±laÅŸtÄ±rma veri seti ile LLM veri seti farklÄ±.
SolidGoldMagikarp sayÄ±sÄ± (token), LLM veri setinde yok.
Bu yÃ¼zden girdi olarak verildiÄŸinde C dilinde oluÅŸan atanmamÄ±ÅŸ bellek (Un-allocated memory) hatasÄ± gibi bir ÅŸey oluyor.
Daha fazlasÄ± iÃ§in bakÄ±nÄ±z: 

- [SolidGoldMagikarp article](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)
- https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/
- https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation
- https://www.lesswrong.com/posts/Ya9LzwEbfaAMY8ABo/solidgoldmagikarp-ii-technical-details-and-more-recent
- https://aizi.substack.com/p/explaining-solidgoldmagikarp-by-looking
- https://deconstructing.ai/deconstructing-ai%E2%84%A2-blog/f/the-enigma-of-solidgoldmagikarp-ais-strangest-token



## LLM Modellerinin Tokenizasyon SÃ¶zlÃ¼k BoyutlarÄ± 

ChatGPT 5 ile Ã¼retilmiÅŸtir.

{{< include ./tables/table-tokenization-vocabulary-sizes-of-llm-models-en.md >}}



## Ã‡ift bayt kodlama (Byte pair encoding)

https://en.wikipedia.org/wiki/Byte-pair_encoding
https://www.geeksforgeeks.org/nlp/byte-pair-encoding-bpe-in-nlp/

### Byte pair encoding: NasÄ±l Ã§alÄ±ÅŸÄ±r

https://github.com/karpathy/minbpe/blob/master/exercise.md

## Kaynak Vidyolar

[Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)


## KÃ¼tÃ¼phaneler

- https://github.com/google/sentencepiece
- https://github.com/openai/tiktoken
- https://github.com/karpathy/minbpe/

