# Sayılaştırma (Tokenization)

Bilişim sözlüğü çeviri olarak simgeleştirme vermiş ama bana göre sayılaştırma LLM bağlamında daha anlamlı.


## Niçin

Tüm makine öğrenme yöntemleri gibi, yapay sinir ağları da sadece sayılar ile çalıştığı için verilen metinleri sayılara çevirmek gerekiyor.


{{< include ./figures/tokenize-workflow.mermaid >}}

## Görselleştirme 


- [çevrimiçi tiktokenizer](https://tiktokenizer.vercel.app/?model=gpt2)

![](../images/tiktokenizer.vercel.app.png)

![](../images/tiktokenizer.vercel.app-example-tr.png)


- [sadece gpt2 görselleştirme aracı](https://github.com/sinjoysaha/tiktokenizer-js) kodu açık ve internet olmadan çalıştırılabiliyor.


### Örnek simgeleştirme yazısı

```txt
{{< include ./src/tokenization-example-string-Andrej-Karpathy.txt >}}
```

## Neden önemli?

Andrej Karpathy 2 saatlik [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE) sunumunda aşağıdaki cümle ile başlamaktadır ve arkasından aşağıdaki tablo verilmektedir. 


	Sayılaştırma (Tokenization), LLM'lerin birçok tuhaflığının merkezinde yer alır. 
	Bunu göz ardı etmeyin.


{{< include ./tables/table-tokenization-karpathy-tr.md >}}


## Sayılaştırma Türleri


{{< include ./figures/tokenization-types-en.mermaid >}}


1. Karakter tabanlı

	- unicode
	- n-grams (karakter tabanlı 1-n grams)

2. Kelime Tabanlı

	- Kelimeden tamsayılara (Word to integers). Örnek kod: SimpleTokenizerV1
	- Bag of words (uni-gram word based)
	- one-hot-encoding

3. alt kelime (hece benzeri) tabanlı

	- Byte pair encoding (GPT versions)
	- Sentence piece (LLama versions)

Sözlük büyük olduğunda, kodlanmış sayılar daha kısa olur.
Çünkü çoğu sözlük, en çok kullanılan kelimeleri tek bir sayıya kodlamak için frekans analizi kullanır.
Bu durumda, LLM parametre sayısı daha az olacaktır.


Sözlük küçük olduğunda, girdi yazısı daha uzun bir sayı kümesi ile kodlanacaktır.
LLM parametre sayısı azalır ama bir sonraki sayı (token) tahmini zorlaşır.


### Unicode kendisi kullanma

- Sözlük geniş
- Unicode standardı canlı ve değişiyor. 
- Bu durumda boyutları değişiyor.
- Yeni gelen Unicode sembollerde sorun yaşanacaktır.

Örneğin [Anadolu hiyeroglifleri](https://en.wikipedia.org/wiki/Anatolian_Hieroglyphs_(Unicode_block)), [emojiler](https://www.unicode.org/emoji/charts/full-emoji-list.html) ...  eklendi

- 𔐀‎ 	𔐁‎ 	𔐂‎ 	𔐃‎ 	𔐄‎ 	𔐅
- 👀 	👁️ 	👂 	👃 	👄 	👅 	👆 	👇 	👈 	👉 	👊 	👋 	👌 	👍 	👎 	👏

utf8,utf16,utf32 gibi farklı standartlar var.

- [unicode wikipedia](https://en.wikipedia.org/wiki/Unicode)
Sürüm 16.0, çeşitli sıradan, edebi, akademik ve teknik bağlamlarda kullanılan 154.998 karakteri ve 168 betiği tanımlar.

## Notebook SimpleTokenizer

Example notebook for [SimpleTokenizer](https://github.com/ati-ozgur/course-nn-deep-learning/blob/master/notebooks/tokenizer-simple.ipynb)


## Özel Simgeler (sayılar)

{{< include ./tables/table-tokenization-special-tokens-tr.md >}}

GPT2 dolgulama ve yazı bitişi için aynı simge olan <|endoftext|> kullanmaktadır.
GPT2, sayılaştırma algoritması olarak BPE kullandığı için [UNK] bilinmeyen simgesine ihtiyaç duymamaktadır.



{{< include ./tokenization-bpe-tr.md >}}


